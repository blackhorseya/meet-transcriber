/**
 * Offscreen Document
 * MV3 æ¶æ§‹ï¼šè² è²¬éŸ³è¨Šæ•ç²å’Œå³æ™‚è½‰éŒ„
 *
 * æµç¨‹ï¼š
 * 1. æ¥æ”¶ START_CAPTURE è¨Šæ¯ï¼ˆåŒ…å« streamId, apiKey, languageï¼‰
 * 2. ä½¿ç”¨ getUserMedia + chromeMediaSource: 'tab' å–å¾— Tab éŸ³è¨Š
 * 3. å¦‚æœå•Ÿç”¨éº¥å…‹é¢¨ï¼Œç”¨ AudioContext æ··åˆå…©å€‹éŸ³æº
 * 4. å»ºç«‹ AudioContext å›æ”¾ï¼ˆè®“ä½¿ç”¨è€…èƒ½è½åˆ°å°æ–¹è²éŸ³ï¼‰
 * 5. æ¯éš” N ç§’é‡å•Ÿ MediaRecorderï¼Œç”¢ç”Ÿå®Œæ•´çš„éŸ³è¨Šæª”
 * 6. å°‡éŸ³è¨Šé€åˆ° Groq Whisper API é€²è¡Œè½‰éŒ„
 * 7. å°‡è½‰éŒ„çµæœé€é message å‚³å› background â†’ content script
 */

import type { StartCapture, TranscriptResult } from '@/lib/message-types';
import { transcribeAudio, isTranscriptionError } from '@/lib/transcription';

console.log('[Offscreen] === OFFSCREEN DOCUMENT LOADED ===');

// ============================================
// ç‹€æ…‹
// ============================================

let mediaRecorder: MediaRecorder | null = null;
let tabStream: MediaStream | null = null;
let micStream: MediaStream | null = null;
let mixingContext: AudioContext | null = null;
let playbackContext: AudioContext | null = null;
let streamToRecord: MediaStream | null = null;

// è½‰éŒ„ç›¸é—œç‹€æ…‹
let apiKey: string = '';
let language: string = 'zh';
let sequenceNumber: number = 0;
let isCapturing: boolean = false;

// éŒ„è£½å¾ªç’°ç‹€æ…‹
let recordingTimer: ReturnType<typeof setTimeout> | null = null;
let currentChunks: Blob[] = [];

// ============================================
// è¨­å®š
// ============================================

// æ¯éš”å¤šå°‘æ¯«ç§’é‡å•ŸéŒ„è£½ä¸¦é€å‡ºè½‰éŒ„ï¼ˆ2-5 ç§’ï¼‰
const RECORDING_DURATION_MS = 3000;

// ============================================
// Message Handling
// ============================================

chrome.runtime.onMessage.addListener((msg: unknown) => {
  const message = msg as { type: string };
  console.log('[Offscreen] Received message:', message.type);

  switch (message.type) {
    case 'START_CAPTURE':
      handleStartCapture(msg as StartCapture);
      break;
    case 'STOP_CAPTURE':
      handleStopCapture();
      break;
  }

  return false;
});

// ============================================
// éŒ„è£½å¾ªç’°
// ============================================

/**
 * é–‹å§‹ä¸€å€‹éŒ„è£½é€±æœŸ
 */
function startRecordingCycle(): void {
  if (!streamToRecord || !isCapturing) {
    console.log('[Offscreen] Cannot start recording cycle - no stream or not capturing');
    return;
  }

  currentChunks = [];

  mediaRecorder = new MediaRecorder(streamToRecord, { mimeType: 'audio/webm;codecs=opus' });

  mediaRecorder.ondataavailable = (event) => {
    if (event.data.size > 0) {
      currentChunks.push(event.data);
      console.log(`[Offscreen] Chunk: ${event.data.size} bytes`);
    }
  };

  mediaRecorder.onstop = async () => {
    if (currentChunks.length === 0) {
      console.log('[Offscreen] No chunks recorded in this cycle');
      return;
    }

    // ğŸ”‘ åˆä½µ chunks æˆå®Œæ•´çš„ WebM æª”æ¡ˆï¼ˆå› ç‚ºæ˜¯å–®æ¬¡éŒ„è£½ï¼Œæ‰€ä»¥æœ‰å®Œæ•´ headerï¼‰
    const audioBlob = new Blob(currentChunks, { type: 'audio/webm' });
    console.log('[Offscreen] Recording cycle complete:', audioBlob.size, 'bytes');

    // éåŒæ­¥è™•ç†è½‰éŒ„ï¼Œä¸é˜»å¡ä¸‹ä¸€å€‹é€±æœŸ
    processAudioBlob(audioBlob);
  };

  mediaRecorder.onerror = (event) => {
    console.error('[Offscreen] MediaRecorder error:', event);
  };

  // é–‹å§‹éŒ„è£½
  mediaRecorder.start();
  console.log('[Offscreen] Recording cycle started');

  // è¨­å®šå®šæ™‚å™¨ï¼Œåœ¨ N ç§’å¾Œåœæ­¢ä¸¦é–‹å§‹ä¸‹ä¸€å€‹é€±æœŸ
  recordingTimer = setTimeout(() => {
    if (mediaRecorder && mediaRecorder.state === 'recording') {
      mediaRecorder.stop();

      // å¦‚æœé‚„åœ¨æ•ç²ä¸­ï¼Œå•Ÿå‹•ä¸‹ä¸€å€‹é€±æœŸ
      if (isCapturing) {
        startRecordingCycle();
      }
    }
  }, RECORDING_DURATION_MS);
}

/**
 * è™•ç†éŸ³è¨Šä¸¦é€åˆ° API è½‰éŒ„
 */
async function processAudioBlob(audioBlob: Blob): Promise<void> {
  // éæ¿¾å¤ªå°çš„éŸ³è¨Š
  if (audioBlob.size < 5000) {
    console.log('[Offscreen] Audio too small, skipping:', audioBlob.size, 'bytes');
    return;
  }

  // æª¢æŸ¥ apiKey
  if (!apiKey) {
    console.log('[Offscreen] No API key, skipping transcription');
    return;
  }

  console.log('[Offscreen] Sending to API:', audioBlob.size, 'bytes');

  const result = await transcribeAudio(audioBlob, apiKey, language);

  if (isTranscriptionError(result)) {
    console.warn('[Offscreen] Transcription failed:', result.error);
    return;
  }

  // éæ¿¾ç©ºç™½çµæœ
  if (!result.text || result.text.trim().length === 0) {
    console.log('[Offscreen] Empty transcription, skipping');
    return;
  }

  console.log('[Offscreen] Transcription result:', result.text);

  // ç™¼é€è½‰éŒ„çµæœåˆ° background
  const transcriptMsg: TranscriptResult = {
    type: 'TRANSCRIPT_RESULT',
    text: result.text,
    timestamp: Date.now(),
    sequenceNumber: sequenceNumber++,
    isFinal: true,
  };

  chrome.runtime.sendMessage(transcriptMsg).catch((err) => {
    console.warn('[Offscreen] Failed to send transcript:', err);
  });
}

// ============================================
// Capture Control
// ============================================

async function handleStartCapture(message: StartCapture): Promise<void> {
  if (isCapturing) {
    console.log('[Offscreen] Already capturing, ignoring start request');
    return;
  }

  const { streamId, includeMicrophone, microphoneDeviceLabel } = message;

  // å„²å­˜ API è¨­å®š
  apiKey = message.apiKey;
  language = message.language || 'zh';
  sequenceNumber = 0;

  console.log('[Offscreen] Starting capture:', {
    streamId: streamId.substring(0, 20) + '...',
    includeMicrophone,
    microphoneDeviceLabel,
    language,
    hasApiKey: !!apiKey,
  });

  if (!apiKey) {
    console.error('[Offscreen] No API key provided');
    chrome.runtime.sendMessage({
      type: 'CAPTURE_ERROR',
      error: 'è«‹å…ˆè¨­å®š Groq API Key',
    }).catch(console.error);
    return;
  }

  try {
    // ğŸ”‘ Step 1: å–å¾— Tab éŸ³è¨Šï¼ˆå°æ–¹çš„è²éŸ³ï¼‰
    tabStream = await navigator.mediaDevices.getUserMedia({
      audio: {
        // @ts-expect-error - Chrome-specific constraints
        mandatory: {
          chromeMediaSource: 'tab',
          chromeMediaSourceId: streamId,
        },
      },
    });

    console.log('[Offscreen] Got Tab stream:', {
      tracks: tabStream.getAudioTracks().length,
      enabled: tabStream.getAudioTracks()[0]?.enabled,
    });

    // ğŸ”‘ Step 2: å›æ”¾ Tab éŸ³è¨Šï¼Œè®“ä½¿ç”¨è€…èƒ½è½åˆ°å°æ–¹è²éŸ³
    playbackContext = new AudioContext();
    if (playbackContext.state === 'suspended') {
      await playbackContext.resume();
    }
    const playbackSource = playbackContext.createMediaStreamSource(tabStream);
    playbackSource.connect(playbackContext.destination);
    console.log('[Offscreen] Tab audio playback connected, state:', playbackContext.state);

    // ğŸ”‘ Step 3: æ±ºå®šè¦éŒ„è£½çš„ stream
    if (includeMicrophone) {
      console.log('[Offscreen] Attempting to get microphone...');
      try {
        // ğŸ”‘ æ ¹æ“š label æ‰¾åˆ°å°æ‡‰çš„ deviceId
        let micDeviceId: string | undefined;
        if (microphoneDeviceLabel) {
          const devices = await navigator.mediaDevices.enumerateDevices();
          const micDevice = devices.find(
            (d) => d.kind === 'audioinput' && d.label === microphoneDeviceLabel
          );
          if (micDevice) {
            micDeviceId = micDevice.deviceId;
            console.log('[Offscreen] Found microphone device:', {
              label: micDevice.label,
              deviceId: micDeviceId.substring(0, 20) + '...',
            });
          } else {
            console.warn(
              '[Offscreen] Microphone device not found by label:',
              microphoneDeviceLabel
            );
          }
        }

        // å–å¾—éº¥å…‹é¢¨ï¼ˆä½¿ç”¨æŒ‡å®šçš„ deviceId æˆ–é è¨­ï¼‰
        micStream = await navigator.mediaDevices.getUserMedia({
          audio: {
            deviceId: micDeviceId ? { exact: micDeviceId } : undefined,
            echoCancellation: false, // ä¸è¦æ¶ˆé™¤å›éŸ³ï¼Œæˆ‘å€‘æƒ³éŒ„åˆ°è‡ªå·±çš„è²éŸ³
            noiseSuppression: true,
            autoGainControl: true,
          },
        });

        console.log('[Offscreen] Got Microphone stream:', {
          tracks: micStream.getAudioTracks().length,
          enabled: micStream.getAudioTracks()[0]?.enabled,
          label: micStream.getAudioTracks()[0]?.label,
        });

        // ğŸ”‘ Step 4: æ··åˆ Tab + éº¥å…‹é¢¨
        mixingContext = new AudioContext();

        // ğŸ”‘ é—œéµï¼šç¢ºä¿ AudioContext æ˜¯ running ç‹€æ…‹
        if (mixingContext.state === 'suspended') {
          await mixingContext.resume();
        }
        console.log('[Offscreen] Mixing AudioContext state:', mixingContext.state);

        const mixedDest = mixingContext.createMediaStreamDestination();

        // é€£æ¥ Tab éŸ³è¨Šåˆ°æ··éŸ³ç›®æ¨™
        const tabSource = mixingContext.createMediaStreamSource(tabStream);
        const tabGain = mixingContext.createGain();
        tabGain.gain.value = 1.0;
        tabSource.connect(tabGain);
        tabGain.connect(mixedDest);

        // é€£æ¥éº¥å…‹é¢¨åˆ°æ··éŸ³ç›®æ¨™
        const micSource = mixingContext.createMediaStreamSource(micStream);
        const micGain = mixingContext.createGain();
        micGain.gain.value = 1.0;
        micSource.connect(micGain);
        micGain.connect(mixedDest);

        console.log('[Offscreen] Audio mixing complete');

        streamToRecord = mixedDest.stream;
      } catch (micErr) {
        console.warn('[Offscreen] Microphone access failed, falling back to tab-only:', micErr);
        streamToRecord = tabStream;
      }
    } else {
      console.log('[Offscreen] Microphone disabled, using tab-only');
      streamToRecord = tabStream;
    }

    // ğŸ”‘ Step 5: é–‹å§‹éŒ„è£½å¾ªç’°
    isCapturing = true;
    startRecordingCycle();

    chrome.runtime.sendMessage({ type: 'CAPTURE_STARTED' }).catch(console.error);
    window.location.hash = 'recording';

    console.log('[Offscreen] Capture started with', RECORDING_DURATION_MS, 'ms cycles');
  } catch (err) {
    console.error('[Offscreen] Failed to start capture:', err);
    chrome.runtime.sendMessage({
      type: 'CAPTURE_ERROR',
      error: err instanceof Error ? err.message : 'ç„¡æ³•å–å¾—éŸ³è¨Š',
    }).catch(console.error);
  }
}

function handleStopCapture(): void {
  console.log('[Offscreen] Stopping capture');

  isCapturing = false;

  // åœæ­¢å®šæ™‚å™¨
  if (recordingTimer) {
    clearTimeout(recordingTimer);
    recordingTimer = null;
  }

  // åœæ­¢ç•¶å‰éŒ„è£½
  if (mediaRecorder && mediaRecorder.state === 'recording') {
    mediaRecorder.stop();
  }
  mediaRecorder = null;

  // åœæ­¢ Tab éŸ³è»Œ
  if (tabStream) {
    tabStream.getTracks().forEach((t) => t.stop());
    tabStream = null;
  }

  // åœæ­¢éº¥å…‹é¢¨éŸ³è»Œ
  if (micStream) {
    micStream.getTracks().forEach((t) => t.stop());
    micStream = null;
  }

  // é—œé–‰ AudioContext
  if (mixingContext && mixingContext.state !== 'closed') {
    mixingContext.close().catch(console.error);
    mixingContext = null;
  }

  if (playbackContext && playbackContext.state !== 'closed') {
    playbackContext.close().catch(console.error);
    playbackContext = null;
  }

  streamToRecord = null;
  apiKey = '';
  currentChunks = [];

  window.location.hash = '';

  chrome.runtime.sendMessage({ type: 'CAPTURE_STOPPED' }).catch(console.error);
}

console.log('[Offscreen] Ready to receive capture commands');
